{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp.dsl import pipeline\n",
    "from kfp.dsl import component\n",
    "from kfp.dsl import OutputPath\n",
    "from kfp.dsl import InputPath\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        Markdown,\n",
    "                        Condition)\n",
    "\n",
    "\n",
    "from kfp.dsl import Output\n",
    "from kfp.dsl import Metrics\n",
    "\n",
    "from kfp import compiler\n",
    "#from kfp.google.client import AIPlatformClient\n",
    "\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "#from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from typing import NamedTuple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"ml-pipelines-project-433602\"\n",
    "PIPELINE_ROOT = \"gs://sb-vertex-temp/\"\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", \n",
    "    packages_to_install = [\n",
    "        \"pandas==2.0.0\",\n",
    "        \"scikit-learn==1.5.1\",\n",
    "        \"numpy==1.23.0\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "def get_data(\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset],\n",
    "):\n",
    "\n",
    "    from sklearn import datasets\n",
    "    from sklearn.model_selection import train_test_split as tts\n",
    "    import pandas as pd\n",
    "\n",
    "\n",
    "    # dataset https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\n",
    "    data_raw = datasets.load_breast_cancer()\n",
    "    data = pd.DataFrame(data_raw.data, columns=data_raw.feature_names)\n",
    "    data[\"target\"] = data_raw.target\n",
    "\n",
    "    train, test = tts(data, test_size=0.3)\n",
    "\n",
    "    train.to_csv(dataset_train.path)\n",
    "    test.to_csv(dataset_test.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", \n",
    "    packages_to_install = [\n",
    "        \"pandas==2.0.0\",\n",
    "        \"numpy==1.23.0\",\n",
    "        \"xgboost==1.7.5\",\n",
    "        \"scikit-learn==1.5.1\", #xgboost requires scikitlearn\n",
    "    ],\n",
    ")\n",
    "def train_model(\n",
    "    dataset: Input[Dataset],\n",
    "    model_artifact: Output[Model]\n",
    "):\n",
    "\n",
    "    from xgboost import XGBClassifier\n",
    "    import pandas as pd\n",
    "\n",
    "    data = pd.read_csv(dataset.path)\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        objective=\"binary:logistic\"\n",
    "    )\n",
    "    model.fit(\n",
    "        data.drop(columns=[\"target\"]),\n",
    "        data.target,\n",
    "    )\n",
    "\n",
    "    score = model.score(\n",
    "        data.drop(columns=[\"target\"]),\n",
    "        data.target,\n",
    "    )\n",
    "\n",
    "    model_artifact.metadata[\"train_score\"] = float(score)\n",
    "    model_artifact.metadata[\"framework\"] = \"XGBoost\"\n",
    "\n",
    "    print(model_artifact.path)\n",
    "\n",
    "    model.save_model(model_artifact.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", \n",
    "    packages_to_install = [\n",
    "        \"pandas==2.0.0\",\n",
    "        \"numpy==1.23.0\",\n",
    "        \"xgboost==1.7.5\",\n",
    "        \"scikit-learn==1.5.1\", #xgboost requires scikitlearn\n",
    "    ],\n",
    ")\n",
    "def eval_model(\n",
    "    test_set: Input[Dataset],\n",
    "    xgb_model: Input[Model],\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "    smetrics: Output[Metrics]\n",
    ") -> NamedTuple(\"Outputs\", [(\"deploy\", str)]):\n",
    "    from xgboost import XGBClassifier\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    data = pd.read_csv(test_set.path)\n",
    "    model = XGBClassifier()\n",
    "    model.load_model(xgb_model.path)\n",
    "\n",
    "    score = model.score(\n",
    "        data.drop(columns=[\"target\"]),\n",
    "        data.target,\n",
    "    )\n",
    "\n",
    "    from sklearn.metrics import roc_curve\n",
    "    y_scores =  model.predict_proba(data.drop(columns=[\"target\"]))[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(\n",
    "         y_true=data.target.to_numpy(), y_score=y_scores, pos_label=True\n",
    "    )\n",
    "    # Create a mask for non-Infinity thresholds\n",
    "    valid_mask = ~np.isinf(thresholds)\n",
    "    \n",
    "    # Apply the mask to filter out Infinity values\n",
    "    fpr = fpr[valid_mask]\n",
    "    tpr = tpr[valid_mask]\n",
    "    thresholds = thresholds[valid_mask]\n",
    "    \n",
    "    metrics.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    y_pred = model.predict(data.drop(columns=[\"target\"]))\n",
    "\n",
    "    metrics.log_confusion_matrix(\n",
    "       [\"False\", \"True\"],\n",
    "       confusion_matrix(\n",
    "           data.target, y_pred\n",
    "       ).tolist()\n",
    "    )\n",
    "\n",
    "    xgb_model.metadata[\"test_score\"] = float(score)\n",
    "    smetrics.log_metric(\"score\", float(score))\n",
    "\n",
    "\n",
    "    deploy = \"true\"\n",
    "    #compare threshold or to previous\n",
    "\n",
    "    return (deploy,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", packages_to_install=[\"google-cloud-aiplatform==1.3.0\"])\n",
    "def deploy(\n",
    "    model: Input[Model],\n",
    "    project: str,\n",
    "    region: str,):\n",
    "\n",
    "  import logging\n",
    "  from google.cloud import aiplatform\n",
    "  aiplatform.init(project=project, location=region)\n",
    "\n",
    "  logging.basicConfig(level=logging.DEBUG)\n",
    "  logging.debug(model)\n",
    "\n",
    "  print(model)\n",
    "  print(model.uri)\n",
    "\n",
    "  import os\n",
    "  path,file = os.path.split(model.uri)\n",
    "\n",
    "  import datetime\n",
    "\n",
    "  # datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "  # serving image https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#xgboost\n",
    "  deployed_model = aiplatform.Model.upload(\n",
    "        display_name=\"xgboost-pipeline\",\n",
    "        artifact_uri = path,\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-4:latest\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m0/zbb6htc128l81r3p8lhs13xm0000gn/T/ipykernel_28376/3276517107.py:15: DeprecationWarning: dsl.Condition is deprecated. Please use dsl.If instead.\n",
      "  with Condition(\n"
     ]
    }
   ],
   "source": [
    "@pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    # pipeline_root=PIPELINE_ROOT + \"xgboost-pipeline\",\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"xgboost-pipeline-with-deployment\",\n",
    ")\n",
    "def pipeline():\n",
    "    dataset_op = get_data()\n",
    "    training_op = train_model(dataset=dataset_op.outputs[\"dataset_train\"])\n",
    "    eval_op = eval_model(\n",
    "        test_set=dataset_op.outputs[\"dataset_test\"],\n",
    "        xgb_model=training_op.outputs[\"model_artifact\"]\n",
    "    )\n",
    "\n",
    "    with Condition(\n",
    "        eval_op.outputs[\"deploy\"] == \"true\",\n",
    "        name=\"deploy\",\n",
    "    ):\n",
    "\n",
    "        deploy_op = deploy(model=training_op.outputs[\"model_artifact\"],\n",
    "                          project=PROJECT_ID,\n",
    "                          region=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path='xgb_pipeline.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/553515216022/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20240825135602\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/553515216022/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20240825135602')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/xgboost-pipeline-with-deployment-20240825135602?project=553515216022\n",
      "PipelineJob projects/553515216022/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20240825135602 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/553515216022/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20240825135602 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/553515216022/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20240825135602 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/553515216022/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20240825135602 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \"  The DAG failed because some tasks failed. The failed tasks are: [condition-1].; Job (project_id = ml-pipelines-project-433602, job_id = 6689108510627594240) is failed due to the above error.; Failed to handle the job: {project_number = 553515216022, job_id = 6689108510627594240}\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m job \u001b[38;5;241m=\u001b[39m pipeline_jobs\u001b[38;5;241m.\u001b[39mPipelineJob(\n\u001b[1;32m      2\u001b[0m     display_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxgb-pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     template_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxgb_pipeline.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m )\n\u001b[0;32m----> 6\u001b[0m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Docs/Study/code/ml_pipelines_kfp/kfp-env/.venv/lib/python3.9/site-packages/google/cloud/aiplatform/pipeline_jobs.py:334\u001b[0m, in \u001b[0;36mPipelineJob.run\u001b[0;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout, enable_preflight_validations)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m        Optional. Whether to enable preflight validations for the PipelineJob.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    332\u001b[0m network \u001b[38;5;241m=\u001b[39m network \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mnetwork\n\u001b[0;32m--> 334\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_preflight_validations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_preflight_validations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Docs/Study/code/ml_pipelines_kfp/kfp-env/.venv/lib/python3.9/site-packages/google/cloud/aiplatform/base.py:863\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    862\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    866\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Desktop/Docs/Study/code/ml_pipelines_kfp/kfp-env/.venv/lib/python3.9/site-packages/google/cloud/aiplatform/pipeline_jobs.py:382\u001b[0m, in \u001b[0;36mPipelineJob._run\u001b[0;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout, enable_preflight_validations)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to ensure network synchronization and to run\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03mthe configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m        Optional. Whether to enable preflight validations for the PipelineJob.\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m    375\u001b[0m     service_account\u001b[38;5;241m=\u001b[39mservice_account,\n\u001b[1;32m    376\u001b[0m     network\u001b[38;5;241m=\u001b[39mnetwork,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m     enable_preflight_validations\u001b[38;5;241m=\u001b[39menable_preflight_validations,\n\u001b[1;32m    380\u001b[0m )\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# AutoSxS view model evaluations\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m details \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_details:\n",
      "File \u001b[0;32m~/Desktop/Docs/Study/code/ml_pipelines_kfp/kfp-env/.venv/lib/python3.9/site-packages/google/cloud/aiplatform/pipeline_jobs.py:793\u001b[0m, in \u001b[0;36mPipelineJob._block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;66;03m# Error is only populated when the job state is\u001b[39;00m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;66;03m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01min\u001b[39;00m _PIPELINE_ERROR_STATES:\n\u001b[0;32m--> 793\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob failed with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39merror)\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    795\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_completed_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \"  The DAG failed because some tasks failed. The failed tasks are: [condition-1].; Job (project_id = ml-pipelines-project-433602, job_id = 6689108510627594240) is failed due to the above error.; Failed to handle the job: {project_number = 553515216022, job_id = 6689108510627594240}\"\n"
     ]
    }
   ],
   "source": [
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"xgb-pipeline\",\n",
    "    template_path=\"xgb_pipeline.json\"\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
