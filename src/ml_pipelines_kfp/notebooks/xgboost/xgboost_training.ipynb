{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp.dsl import pipeline\n",
    "from kfp.dsl import component\n",
    "from kfp.dsl import OutputPath\n",
    "from kfp.dsl import InputPath\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        Markdown,\n",
    "                        Condition)\n",
    "\n",
    "\n",
    "from kfp.dsl import Output\n",
    "from kfp.dsl import Metrics\n",
    "\n",
    "from kfp import compiler\n",
    "#from kfp.google.client import AIPlatformClient\n",
    "\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "#from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from typing import NamedTuple\n",
    "\n",
    "import os\n",
    "from google.oauth2 import service_account\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to service account key file\n",
    "key_path = \"/Users/shlba/Desktop/Docs/Study/code/ml_pipelines_kfp/deeplearning-sahil-e50332de6687.json\"\n",
    "\n",
    "\n",
    "# Create credentials using service account key file\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    key_path,\n",
    "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"deeplearning-sahil\"\n",
    "PIPELINE_ROOT = \"gs://sb-vertex/\"\n",
    "REGION = \"us-central1\"\n",
    "SERVICE_ACCOUNT = \"kfp-mlops@deeplearning-sahil.iam.gserviceaccount.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", \n",
    "    packages_to_install = [\n",
    "        \"pandas==2.0.0\",\n",
    "        \"scikit-learn==1.5.1\",\n",
    "        \"numpy==1.23.0\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "def get_data(\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset],\n",
    "):\n",
    "\n",
    "    from sklearn import datasets\n",
    "    from sklearn.model_selection import train_test_split as tts\n",
    "    import pandas as pd\n",
    "\n",
    "\n",
    "    # dataset https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\n",
    "    data_raw = datasets.load_breast_cancer()\n",
    "    data = pd.DataFrame(data_raw.data, columns=data_raw.feature_names)\n",
    "    data[\"target\"] = data_raw.target\n",
    "\n",
    "    train, test = tts(data, test_size=0.3)\n",
    "\n",
    "    train.to_csv(dataset_train.path)\n",
    "    test.to_csv(dataset_test.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", \n",
    "    packages_to_install = [\n",
    "        \"pandas==2.0.0\",\n",
    "        \"numpy==1.23.0\",\n",
    "        \"xgboost==1.7.5\",\n",
    "        \"scikit-learn==1.5.1\", #xgboost requires scikitlearn\n",
    "    ],\n",
    ")\n",
    "def train_model(\n",
    "    dataset: Input[Dataset],\n",
    "    model: Output[Model]\n",
    "):\n",
    "\n",
    "    from xgboost import XGBClassifier\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    import logging\n",
    "\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "    data = pd.read_csv(dataset.path)\n",
    "\n",
    "    xgb_model = XGBClassifier(\n",
    "        objective=\"binary:logistic\"\n",
    "    )\n",
    "    xgb_model.fit(\n",
    "        data.drop(columns=[\"target\"]),\n",
    "        data.target,\n",
    "    )\n",
    "\n",
    "    score = xgb_model.score(\n",
    "        data.drop(columns=[\"target\"]),\n",
    "        data.target,\n",
    "    )\n",
    "\n",
    "    model.metadata[\"train_score\"] = float(score)\n",
    "    model.metadata[\"framework\"] = \"XGBoost\"\n",
    "\n",
    "    print(model.path)\n",
    "\n",
    "    #model.save_model(model_artifact.path)\n",
    "    joblib.dump(xgb_model, model.path+'.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", \n",
    "    packages_to_install = [\n",
    "        \"pandas==2.0.0\",\n",
    "        \"numpy==1.23.0\",\n",
    "        \"xgboost==1.7.5\",\n",
    "        \"scikit-learn==1.5.1\", #xgboost requires scikitlearn\n",
    "    ],\n",
    ")\n",
    "def eval_model(\n",
    "    test_set: Input[Dataset],\n",
    "    xgb_model: Input[Model],\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "    smetrics: Output[Metrics]\n",
    ") -> NamedTuple(\"Outputs\", [(\"deploy\", str)]):\n",
    "    from xgboost import XGBClassifier\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "\n",
    "    data = pd.read_csv(test_set.path)\n",
    "    #model = XGBClassifier()\n",
    "    #model.load_model(xgb_model.path)\n",
    "    model = joblib.load(xgb_model.path+'.joblib')\n",
    "    score = model.score(\n",
    "        data.drop(columns=[\"target\"]),\n",
    "        data.target,\n",
    "    )\n",
    "\n",
    "    from sklearn.metrics import roc_curve\n",
    "    y_scores =  model.predict_proba(data.drop(columns=[\"target\"]))[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(\n",
    "         y_true=data.target.to_numpy(), y_score=y_scores, pos_label=True\n",
    "    )\n",
    "    # Create a mask for non-Infinity thresholds\n",
    "    valid_mask = ~np.isinf(thresholds)\n",
    "    \n",
    "    # Apply the mask to filter out Infinity values\n",
    "    fpr = fpr[valid_mask]\n",
    "    tpr = tpr[valid_mask]\n",
    "    thresholds = thresholds[valid_mask]\n",
    "    \n",
    "    metrics.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    y_pred = model.predict(data.drop(columns=[\"target\"]))\n",
    "\n",
    "    metrics.log_confusion_matrix(\n",
    "       [\"False\", \"True\"],\n",
    "       confusion_matrix(\n",
    "           data.target, y_pred\n",
    "       ).tolist()\n",
    "    )\n",
    "\n",
    "    xgb_model.metadata[\"test_score\"] = float(score)\n",
    "    smetrics.log_metric(\"score\", float(score))\n",
    "\n",
    "\n",
    "    deploy = \"true\"\n",
    "    #compare threshold or to previous\n",
    "\n",
    "    return (deploy,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", packages_to_install=[\"google-cloud-aiplatform==1.3.0\"])\n",
    "def deploy(\n",
    "    model: Input[Model],\n",
    "    project: str,\n",
    "    region: str,):\n",
    "\n",
    "  import logging\n",
    "  from google.cloud import aiplatform\n",
    "  aiplatform.init(project=project, location=region)\n",
    "\n",
    "  logging.basicConfig(level=logging.DEBUG)\n",
    "  logging.debug(model)\n",
    "\n",
    "  print(model)\n",
    "  print(model.uri)\n",
    "\n",
    "  import os\n",
    "  path,file = os.path.split(model.uri)\n",
    "  print(path)\n",
    "  logging.info(path)\n",
    "  import datetime\n",
    "\n",
    "  # datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "  # serving image https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#xgboost\n",
    "  deployed_model = aiplatform.Model.upload(\n",
    "        display_name=\"xgboost-pipeline\",\n",
    "        artifact_uri = path,\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-4:latest\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m0/zbb6htc128l81r3p8lhs13xm0000gn/T/ipykernel_14239/182255530.py:15: DeprecationWarning: dsl.Condition is deprecated. Please use dsl.If instead.\n",
      "  with Condition(\n"
     ]
    }
   ],
   "source": [
    "@pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    # pipeline_root=PIPELINE_ROOT + \"xgboost-pipeline\",\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"xgboost-pipeline-with-deployment\",\n",
    ")\n",
    "def pipeline():\n",
    "    dataset_op = get_data()\n",
    "    training_op = train_model(dataset=dataset_op.outputs[\"dataset_train\"])\n",
    "    eval_op = eval_model(\n",
    "        test_set=dataset_op.outputs[\"dataset_test\"],\n",
    "        xgb_model=training_op.outputs[\"model\"]\n",
    "    )\n",
    "\n",
    "    with Condition(\n",
    "        eval_op.outputs[\"deploy\"] == \"true\",\n",
    "        name=\"deploy\",\n",
    "    ):\n",
    "\n",
    "        deploy_op = deploy(model=training_op.outputs[\"model\"],\n",
    "                          project=PROJECT_ID,\n",
    "                          region=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path='xgb_pipeline.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/57434141298/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20250504233448\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/57434141298/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20250504233448')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/xgboost-pipeline-with-deployment-20250504233448?project=57434141298\n",
      "PipelineJob created. Resource name: projects/57434141298/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20250504233448\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/57434141298/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20250504233448')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/xgboost-pipeline-with-deployment-20250504233448?project=57434141298\n",
      "PipelineJob projects/57434141298/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20250504233448 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n",
      "PipelineJob projects/57434141298/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20250504233448 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n",
      "PipelineJob projects/57434141298/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20250504233448 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/57434141298/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20250504233448 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/57434141298/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20250504233448 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/57434141298/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20250504233448 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/57434141298/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20250504233448 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/57434141298/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20250504233448 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/57434141298/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20250504233448 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/57434141298/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20250504233448 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/57434141298/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20250504233448 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/57434141298/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20250504233448 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/57434141298/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20250504233448\n",
      "PipelineJob run completed. Resource name: projects/57434141298/locations/us-central1/pipelineJobs/xgboost-pipeline-with-deployment-20250504233448\n"
     ]
    }
   ],
   "source": [
    "# Initialize Vertex AI with credentials\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    credentials=credentials\n",
    ")\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"xgb-pipeline\",\n",
    "    template_path=\"xgb_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    credentials=credentials\n",
    ")\n",
    "\n",
    "job.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
