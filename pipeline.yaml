# PIPELINE DEFINITION
# Name: pipeline-iris
# Inputs:
#    bq_dataset: str
#    bq_table: str
#    bq_table_predictions: str
#    location: str
#    project_id: str
components:
  comp-get-model:
    executorLabel: exec-get-model
    inputDefinitions:
      parameters:
        location:
          parameterType: STRING
        model_name:
          parameterType: STRING
        project_id:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        latest_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-inference-model:
    executorLabel: exec-inference-model
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        bq_dataset:
          parameterType: STRING
        bq_table:
          parameterType: STRING
        bq_table_predictions:
          parameterType: STRING
        location:
          parameterType: STRING
        project_id:
          parameterType: STRING
defaultPipelineRoot: gs://sb-vertex/pipeline_root
deploymentSpec:
  executors:
    exec-get-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - get_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.64.0'\
          \ 'fsspec==2024.6.1' 'gcsfs==2024.6.1' 'joblib==1.4.2' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef get_model(\n    project_id: str,\n    location: str,\n    model_name:\
          \ str,\n    latest_model: Output[Model],\n):\n    from google.cloud import\
          \ aiplatform, aiplatform_v1\n    import fsspec\n    import gcsfs\n    import\
          \ joblib\n\n    aiplatform.init(project=project_id, location=location)\n\
          \n\n    client = aiplatform_v1.ModelServiceClient(\n            client_options={\"\
          api_endpoint\": f\"{location}-aiplatform.googleapis.com\"}\n    )\n\n  \
          \  request = {\n        \"parent\": f\"projects/{project_id}/locations/{location}\"\
          ,\n        \"filter\": f\"display_name={model_name}\"\n    }\n    parent_models\
          \ = list(client.list_models(request=request))\n    parent_model = parent_models[0]\
          \ if parent_models else None\n\n    if not parent_model:\n        print(f\"\
          Could not find model with f{model_name}\")\n        return\n\n    print(f\"\
          Parent Model - {parent_model}\")\n    print(f\"class - {type(parent_model)}\"\
          )\n    print(f\"name - {parent_model.name}\")\n    print(f\"model path-\
          \ {parent_model.artifact_uri}\")\n    print(f\"output model path - {latest_model.path}\"\
          )\n    latest_model_path = latest_model.path.replace(\"/gcs/\", \"gs://\"\
          )\n    print(f\"output model path cleaned - {latest_model_path}\")\n   \
          \ fs, _ = fsspec.core.url_to_fs(parent_model.artifact_uri)\n    print(f\"\
          file system: {fs}\")\n    fs.copy(parent_model.artifact_uri+\"/\", latest_model.path.replace(\"\
          /gcs/\", \"gs://\"), recursive=True)\n\n"
        image: python:3.10
    exec-inference-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - inference_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas==2.0.0'\
          \ 'scikit-learn==1.5.1' 'numpy==1.23.0' 'joblib==1.4.2' 'google-cloud-bigquery==2.34.3'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef inference_model(\n    project_id: str,\n    location: str,\n\
          \    bq_dataset: str,\n    bq_table: str,\n    bq_table_predictions: str,\n\
          \    model: Input[Model],\n):\n    import joblib\n    import pandas as pd\n\
          \    import numpy as np\n    from google.cloud import bigquery\n    from\
          \ datetime import datetime\n\n    client = bigquery.Client()\n\n    dataset_ref\
          \ = bigquery.DatasetReference(project_id, bq_dataset)\n    table_ref = dataset_ref.table(bq_table)\n\
          \    table = bigquery.Table(table_ref)\n    iterable_table = client.list_rows(table).to_dataframe_iterable()\n\
          \n    dfs = []\n    for row in iterable_table:\n        dfs.append(row)\n\
          \n    df = pd.concat(dfs, ignore_index=True)\n    df = df.drop(columns=['species'])\n\
          \    print(f\"Model Path: {model.path}\")\n    inf_model = joblib.load(model.path+'/model.joblib')\n\
          \    inf_pred = inf_model.predict(df)\n    print(len(inf_pred))\n    print(inf_pred[:5])\n\
          \n    # Create predictions dataframe\n    predictions_df = df.copy()\n \
          \   predictions_df['prediction'] = inf_pred\n    predictions_df['prediction_timestamp']\
          \ = datetime.now()\n    predictions_df['model_path'] = model.path\n\n  \
          \  # Write predictions to BigQuery\n    predictions_table_id = f\"{project_id}.{bq_dataset}.{bq_table_predictions}\"\
          \n\n    job_config = bigquery.LoadJobConfig(\n        write_disposition=\"\
          WRITE_APPEND\",\n        schema_update_options=[bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION],\n\
          \    )\n\n    job = client.load_table_from_dataframe(\n        predictions_df,\
          \ predictions_table_id, job_config=job_config\n    )\n    job.result() \
          \ # Wait for the job to complete\n\n    print(f\"Loaded {len(predictions_df)}\
          \ rows to {predictions_table_id}\")\n\n"
        image: python:3.10
pipelineInfo:
  name: pipeline-iris
root:
  dag:
    tasks:
      get-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-get-model
        inputs:
          parameters:
            location:
              componentInputParameter: location
            model_name:
              runtimeValue:
                constant: Iris-Classifier-XGBoost
            project_id:
              componentInputParameter: project_id
        taskInfo:
          name: Get Model
      inference-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-inference-model
        dependentTasks:
        - get-model
        inputs:
          artifacts:
            model:
              taskOutputArtifact:
                outputArtifactKey: latest_model
                producerTask: get-model
          parameters:
            bq_dataset:
              componentInputParameter: bq_dataset
            bq_table:
              componentInputParameter: bq_table
            bq_table_predictions:
              componentInputParameter: bq_table_predictions
            location:
              componentInputParameter: location
            project_id:
              componentInputParameter: project_id
        taskInfo:
          name: Inference Model
  inputDefinitions:
    parameters:
      bq_dataset:
        parameterType: STRING
      bq_table:
        parameterType: STRING
      bq_table_predictions:
        parameterType: STRING
      location:
        parameterType: STRING
      project_id:
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.13.0
